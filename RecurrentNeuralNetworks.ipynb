{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks\n",
    "\n",
    "calcular o preço das ações do google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 1 - Data Preprocessing\n",
    "\n",
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[325.25],\n",
       "       [331.27],\n",
       "       [329.83],\n",
       "       ...,\n",
       "       [793.7 ],\n",
       "       [783.33],\n",
       "       [782.75]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing the training set\n",
    "dataset_train = pd.read_csv('Google_Stock_Price_Train.csv')\n",
    "training_set = dataset_train.iloc[:, 1:2].values\n",
    "#dataset_train.head(50)\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.08581368],\n",
       "       [0.09701243],\n",
       "       [0.09433366],\n",
       "       ...,\n",
       "       [0.95725128],\n",
       "       [0.93796041],\n",
       "       [0.93688146]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feature Scaling\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "sc = MinMaxScaler(feature_range = (0, 1))\n",
    "training_set_scaled = sc.fit_transform(training_set)\n",
    "training_set_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a data structure with 60 timesteps and 1 output\n",
    "X_train = []\n",
    "y_train = []\n",
    "for i in range(60, 1258):\n",
    "    X_train.append(training_set_scaled[i-60:i, 0])\n",
    "    y_train.append(training_set_scaled[i, 0])\n",
    "X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "\n",
    "# Reshaping\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/alvarohsg/mnt/c/Users/alvar/Documents/UnB/Conda/envs/ANN/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/alvarohsg/mnt/c/Users/alvar/Documents/UnB/Conda/envs/ANN/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "# Part 2 - Building the RNN\n",
    "\n",
    "# Importing the Keras libraries and packages\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import GRU\n",
    "from keras.layers import Dropout\n",
    "\n",
    "# Initialising the RNN\n",
    "regressor = Sequential()\n",
    "\n",
    "# Adding the first GRU layer and some Dropout regularisation\n",
    "regressor.add(GRU(units = 50, return_sequences = True, input_shape = (X_train.shape[1], 1)))\n",
    "regressor.add(Dropout(0.2))\n",
    "\n",
    "# Adding a second GRU layer and some Dropout regularisation\n",
    "regressor.add(GRU(units = 50, return_sequences = True))\n",
    "regressor.add(Dropout(0.1))\n",
    "\n",
    "# Adding a third GRU layer and some Dropout regularisation\n",
    "regressor.add(GRU(units = 50, return_sequences = True))\n",
    "regressor.add(Dropout(0.2))\n",
    "\n",
    "# Adding a fourth GRU layer and some Dropout regularisation\n",
    "regressor.add(GRU(units = 50))\n",
    "regressor.add(Dropout(0.1))\n",
    "\n",
    "# Adding the output layer\n",
    "regressor.add(Dense(units = 1))\n",
    "\n",
    "# Compiling the RNN\n",
    "regressor.compile(optimizer = 'adam', loss = 'mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/alvarohsg/mnt/c/Users/alvar/Documents/UnB/Conda/envs/ANN/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/150\n",
      "1198/1198 [==============================] - 9s 7ms/step - loss: 0.0184\n",
      "Epoch 2/150\n",
      "1198/1198 [==============================] - 7s 6ms/step - loss: 0.0035\n",
      "Epoch 3/150\n",
      "1198/1198 [==============================] - 7s 6ms/step - loss: 0.0038\n",
      "Epoch 4/150\n",
      "1198/1198 [==============================] - 7s 6ms/step - loss: 0.0030\n",
      "Epoch 5/150\n",
      "1198/1198 [==============================] - 7s 6ms/step - loss: 0.0028\n",
      "Epoch 6/150\n",
      "1198/1198 [==============================] - 7s 6ms/step - loss: 0.0028\n",
      "Epoch 7/150\n",
      "1198/1198 [==============================] - 7s 6ms/step - loss: 0.0029\n",
      "Epoch 8/150\n",
      "1198/1198 [==============================] - 8s 6ms/step - loss: 0.0025\n",
      "Epoch 9/150\n",
      "1198/1198 [==============================] - 8s 6ms/step - loss: 0.0022\n",
      "Epoch 10/150\n",
      "1198/1198 [==============================] - 8s 6ms/step - loss: 0.0021\n",
      "Epoch 11/150\n",
      "1198/1198 [==============================] - 8s 6ms/step - loss: 0.0022\n",
      "Epoch 12/150\n",
      "1198/1198 [==============================] - 8s 6ms/step - loss: 0.0022\n",
      "Epoch 13/150\n",
      "1198/1198 [==============================] - 8s 6ms/step - loss: 0.0021\n",
      "Epoch 14/150\n",
      "1198/1198 [==============================] - 8s 6ms/step - loss: 0.0022\n",
      "Epoch 15/150\n",
      "1198/1198 [==============================] - 8s 6ms/step - loss: 0.0022\n",
      "Epoch 16/150\n",
      "1198/1198 [==============================] - 8s 6ms/step - loss: 0.0017\n",
      "Epoch 17/150\n",
      "1198/1198 [==============================] - 8s 6ms/step - loss: 0.0016\n",
      "Epoch 18/150\n",
      "1198/1198 [==============================] - 8s 6ms/step - loss: 0.0017\n",
      "Epoch 19/150\n",
      "1198/1198 [==============================] - 8s 6ms/step - loss: 0.0016\n",
      "Epoch 20/150\n",
      "1198/1198 [==============================] - 8s 6ms/step - loss: 0.0018\n",
      "Epoch 21/150\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 0.0018\n",
      "Epoch 22/150\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 0.0017\n",
      "Epoch 23/150\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 0.0018\n",
      "Epoch 24/150\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 0.0017\n",
      "Epoch 25/150\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 0.0017\n",
      "Epoch 26/150\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 0.0014\n",
      "Epoch 27/150\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 0.0017\n",
      "Epoch 28/150\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 0.0014\n",
      "Epoch 29/150\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 0.0014\n",
      "Epoch 30/150\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 0.0014\n",
      "Epoch 31/150\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 0.0014\n",
      "Epoch 32/150\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 0.0013\n",
      "Epoch 33/150\n",
      "1198/1198 [==============================] - 9s 7ms/step - loss: 0.0014\n",
      "Epoch 34/150\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 0.0013\n",
      "Epoch 35/150\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 0.0013\n",
      "Epoch 36/150\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 0.0012\n",
      "Epoch 37/150\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 0.0013\n",
      "Epoch 38/150\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 0.0013\n",
      "Epoch 39/150\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 0.0011\n",
      "Epoch 40/150\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 0.0014\n",
      "Epoch 41/150\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 0.0012\n",
      "Epoch 42/150\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 0.0012\n",
      "Epoch 43/150\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 0.0012\n",
      "Epoch 44/150\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 0.0012\n",
      "Epoch 45/150\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 0.0016\n",
      "Epoch 46/150\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 0.0011\n",
      "Epoch 47/150\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 0.0012\n",
      "Epoch 48/150\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 0.0012\n",
      "Epoch 49/150\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 0.0012\n",
      "Epoch 50/150\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 0.0012\n",
      "Epoch 51/150\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 0.0012\n",
      "Epoch 52/150\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 0.0012\n",
      "Epoch 53/150\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 0.0012\n",
      "Epoch 54/150\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 0.0010\n",
      "Epoch 55/150\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 0.0011\n",
      "Epoch 56/150\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 0.0012\n",
      "Epoch 57/150\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 0.0011\n",
      "Epoch 58/150\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 0.0011\n",
      "Epoch 59/150\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 0.0010\n",
      "Epoch 60/150\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 0.0010\n",
      "Epoch 61/150\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 9.5133e-04\n",
      "Epoch 62/150\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 0.0011\n",
      "Epoch 63/150\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 0.0011\n",
      "Epoch 64/150\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 9.1657e-04\n",
      "Epoch 65/150\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 0.0010\n",
      "Epoch 66/150\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 0.0011\n",
      "Epoch 67/150\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 9.1591e-04\n",
      "Epoch 68/150\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 9.3122e-04\n",
      "Epoch 69/150\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 0.0010\n",
      "Epoch 70/150\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 9.8225e-04\n",
      "Epoch 71/150\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 0.0011\n",
      "Epoch 72/150\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 9.4044e-04\n",
      "Epoch 73/150\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 0.0011\n",
      "Epoch 74/150\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 0.0011\n",
      "Epoch 75/150\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 9.7690e-04\n",
      "Epoch 76/150\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 9.7590e-04\n",
      "Epoch 77/150\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 8.8665e-04\n",
      "Epoch 78/150\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 0.0010\n",
      "Epoch 79/150\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 0.0011\n",
      "Epoch 80/150\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 0.0010\n",
      "Epoch 81/150\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 9.5920e-04\n",
      "Epoch 82/150\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 8.6471e-04\n",
      "Epoch 83/150\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 8.9377e-04\n",
      "Epoch 84/150\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 9.6094e-04\n",
      "Epoch 85/150\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 8.6690e-04\n",
      "Epoch 86/150\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 9.0556e-04\n",
      "Epoch 87/150\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 0.0011\n",
      "Epoch 88/150\n",
      "1198/1198 [==============================] - 9s 7ms/step - loss: 8.5806e-04\n",
      "Epoch 89/150\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 9.1377e-04\n",
      "Epoch 90/150\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 0.0011\n",
      "Epoch 91/150\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 8.8016e-04\n",
      "Epoch 92/150\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 9.0349e-04\n",
      "Epoch 93/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1198/1198 [==============================] - 8s 7ms/step - loss: 8.1980e-04\n",
      "Epoch 94/150\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 8.6681e-04\n",
      "Epoch 95/150\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 9.6933e-04\n",
      "Epoch 96/150\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 9.6115e-04\n",
      "Epoch 97/150\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 8.8200e-04\n",
      "Epoch 98/150\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 8.1957e-04\n",
      "Epoch 99/150\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 8.0645e-04\n",
      "Epoch 100/150\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 9.2015e-04\n",
      "Epoch 101/150\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 8.5026e-04\n",
      "Epoch 102/150\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 8.0422e-04\n",
      "Epoch 103/150\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 8.3798e-04\n",
      "Epoch 104/150\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 9.0645e-04\n",
      "Epoch 105/150\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 7.7456e-04\n",
      "Epoch 106/150\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 9.3735e-04\n",
      "Epoch 107/150\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 8.8968e-04\n",
      "Epoch 108/150\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 7.9752e-04\n",
      "Epoch 109/150\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 9.3843e-04\n",
      "Epoch 110/150\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 7.5580e-04\n",
      "Epoch 111/150\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 8.0298e-04\n",
      "Epoch 112/150\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 7.8996e-04\n",
      "Epoch 113/150\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 7.2988e-04\n",
      "Epoch 114/150\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 8.4084e-04\n",
      "Epoch 115/150\n",
      "1198/1198 [==============================] - 9s 7ms/step - loss: 7.6859e-04\n",
      "Epoch 116/150\n",
      "1198/1198 [==============================] - 10s 8ms/step - loss: 7.7955e-04\n",
      "Epoch 117/150\n",
      "1198/1198 [==============================] - 9s 8ms/step - loss: 7.9661e-04\n",
      "Epoch 118/150\n",
      "1198/1198 [==============================] - 10s 8ms/step - loss: 9.0938e-04\n",
      "Epoch 119/150\n",
      "1198/1198 [==============================] - 9s 8ms/step - loss: 7.1509e-04\n",
      "Epoch 120/150\n",
      "1198/1198 [==============================] - 9s 7ms/step - loss: 8.0890e-04\n",
      "Epoch 121/150\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 8.8376e-04\n",
      "Epoch 122/150\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 7.4705e-04\n",
      "Epoch 123/150\n",
      "1198/1198 [==============================] - 9s 7ms/step - loss: 7.8862e-04\n",
      "Epoch 124/150\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 7.4735e-04\n",
      "Epoch 125/150\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 7.5840e-04\n",
      "Epoch 126/150\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 9.2186e-04\n",
      "Epoch 127/150\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 7.1519e-04\n",
      "Epoch 128/150\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 8.0278e-04\n",
      "Epoch 129/150\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 7.5904e-04\n",
      "Epoch 130/150\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 7.6221e-04\n",
      "Epoch 131/150\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 8.0726e-04\n",
      "Epoch 132/150\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 7.4883e-04\n",
      "Epoch 133/150\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 8.4565e-04\n",
      "Epoch 134/150\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 7.7629e-04\n",
      "Epoch 135/150\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 8.8099e-04\n",
      "Epoch 136/150\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 7.6483e-04\n",
      "Epoch 137/150\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 7.8718e-04\n",
      "Epoch 138/150\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 8.6306e-04\n",
      "Epoch 139/150\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 7.7787e-04\n",
      "Epoch 140/150\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 7.2560e-04\n",
      "Epoch 141/150\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 7.1624e-04\n",
      "Epoch 142/150\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 7.6774e-04\n",
      "Epoch 143/150\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 6.7614e-04\n",
      "Epoch 144/150\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 7.1373e-04\n",
      "Epoch 145/150\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 7.0247e-04\n",
      "Epoch 146/150\n",
      " 930/1198 [======================>.......] - ETA: 1s - loss: 7.5176e-04"
     ]
    }
   ],
   "source": [
    "# Fitting the RNN to the Training set\n",
    "regressor.fit(X_train, y_train, epochs = 150, batch_size = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 3 - Making the predictions and visualising the results\n",
    "\n",
    "# Getting the real stock price of 2017\n",
    "dataset_test = pd.read_csv('Google_Stock_Price_Test.csv')\n",
    "real_stock_price = dataset_test.iloc[:, 1:2].values\n",
    "\n",
    "# Getting the predicted stock price of 2017\n",
    "dataset_total = pd.concat((dataset_train['Open'], dataset_test['Open']), axis = 0)\n",
    "inputs = dataset_total[len(dataset_total) - len(dataset_test) - 60:].values\n",
    "inputs = inputs.reshape(-1,1)\n",
    "inputs = sc.transform(inputs)\n",
    "X_test = []\n",
    "for i in range(60, 80):\n",
    "    X_test.append(inputs[i-60:i, 0])\n",
    "X_test = np.array(X_test)\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "predicted_stock_price = regressor.predict(X_test)\n",
    "predicted_stock_price = sc.inverse_transform(predicted_stock_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualising the results\n",
    "plt.plot(real_stock_price, color = 'red', label = 'Real Google Stock Price')\n",
    "plt.plot(predicted_stock_price, color = 'blue', label = 'Predicted Google Stock Price')\n",
    "plt.title('Google Stock Price Prediction')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Google Stock Price')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FIM"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
